{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRAPHRAG_LLM_MODEL=gpt-4o-mini\n",
    "# GRAPHRAG_ENTITY_EXTRACTION_ENTITY_TYPES=organization,person,event,geo\n",
    "# GRAPHRAG_ENTITY_EXTRACTION_MAX_GLEANINGS=1\n",
    "# GRAPHRAG_CLAIM_EXTRACTION_ENABLED=False\n",
    "# GRAPHRAG_CLAIM_EXTRACTION_MAX_GLEANINGS=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully converted Behind the Tech_Episode 30_Collier_Bloomberg_Transcript.docx to Behind the Tech_Episode 30_Collier_Bloomberg_Transcript.txt\n",
      "Successfully converted Behind the Tech_Episode 31_AshleyLlorens_Transcript.docx to Behind the Tech_Episode 31_AshleyLlorens_Transcript.txt\n",
      "Successfully converted Behind the Tech_Episode 32_KimberlyBryant_Transcript.docx to Behind the Tech_Episode 32_KimberlyBryant_Transcript.txt\n",
      "Successfully converted Behind the Tech_Episode 34_ChaseJarvis_Transcript.docx to Behind the Tech_Episode 34_ChaseJarvis_Transcript.txt\n",
      "Successfully converted Behind the Tech_Episode 35_StevenBathiche_Transcript.docx to Behind the Tech_Episode 35_StevenBathiche_Transcript.txt\n",
      "Successfully converted Behind the Tech_Episode 36_PeterLee_Transcript.docx to Behind the Tech_Episode 36_PeterLee_Transcript.txt\n",
      "Successfully converted Behind the Tech_Episode 37_JustineEzarik_Transcript.docx to Behind the Tech_Episode 37_JustineEzarik_Transcript.txt\n",
      "Successfully converted Behind-the-Tech_EP-38_StevenPinker_Transcript.docx to Behind-the-Tech_EP-38_StevenPinker_Transcript.txt\n",
      "Successfully converted Behind-the-Tech_EP39_2021_Year-in-Review_Transcript.docx to Behind-the-Tech_EP39_2021_Year-in-Review_Transcript.txt\n",
      "Successfully converted EP-00_Behind_the_Tech_Kevin Scott_Podcast_Trailer.docx to EP-00_Behind_the_Tech_Kevin Scott_Podcast_Trailer.txt\n",
      "Successfully converted EP-01_ Behind_the_Tech_Kevin Scott_Podcast_with_Anders Hejlsberg.docx to EP-01_ Behind_the_Tech_Kevin Scott_Podcast_with_Anders Hejlsberg.txt\n",
      "Successfully converted EP-02_ Behind_the_Tech_Kevin Scott_Podcast_with_Alice Steinglass.docx to EP-02_ Behind_the_Tech_Kevin Scott_Podcast_with_Alice Steinglass.txt\n",
      "Successfully converted EP-03_ Behind_the_Tech_Kevin Scott_Podcast_with_Andrew Ng.docx to EP-03_ Behind_the_Tech_Kevin Scott_Podcast_with_Andrew Ng.txt\n",
      "Successfully converted EP-04_ Behind_the_Tech_Kevin Scott_Podcast_with_Judy Estrin.docx to EP-04_ Behind_the_Tech_Kevin Scott_Podcast_with_Judy Estrin.txt\n",
      "Successfully converted EP-05_ Behind_the_Tech_Kevin Scott_Podcast_with_Danielle Feinberg.docx to EP-05_ Behind_the_Tech_Kevin Scott_Podcast_with_Danielle Feinberg.txt\n",
      "Successfully converted EP-06_ Behind_the_Tech_Kevin Scott_Podcast_with_Wences_Casares_TRANSCRIPT.docx to EP-06_ Behind_the_Tech_Kevin Scott_Podcast_with_Wences_Casares_TRANSCRIPT.txt\n",
      "Successfully converted EP-07_ Behind_the_Tech_Kevin Scott_Podcast_with_Dio_Gonzalez_TRANSCRIPT.docx to EP-07_ Behind_the_Tech_Kevin Scott_Podcast_with_Dio_Gonzalez_TRANSCRIPT.txt\n",
      "Successfully converted EP-08_ Behind_the_Tech_Kevin Scott_Podcast_with_Jason_Lanier_TRANSCRIPT.docx to EP-08_ Behind_the_Tech_Kevin Scott_Podcast_with_Jason_Lanier_TRANSCRIPT.txt\n",
      "Successfully converted EP-11_ Behind_the_Tech_Kevin Scott_Podcast_with_Fei-Fei-Li.docx to EP-11_ Behind_the_Tech_Kevin Scott_Podcast_with_Fei-Fei-Li.txt\n",
      "Successfully converted EP-12_Behind_the_Tech_Kevin Scott_Podcast_with_Bill-Coughran_TRANSCRIPT.pdf to EP-12_Behind_the_Tech_Kevin Scott_Podcast_with_Bill-Coughran_TRANSCRIPT.txt\n",
      "Successfully converted EP-13_ Behind_the_Tech_Kevin_Scott_Podcast_with_Sam-Altman.docx to EP-13_ Behind_the_Tech_Kevin_Scott_Podcast_with_Sam-Altman.txt\n",
      "Successfully converted EP-14_ Behind_the_Tech_Kevin_Scott_Podcast_with_Neha_Narkhede.docx to EP-14_ Behind_the_Tech_Kevin_Scott_Podcast_with_Neha_Narkhede.txt\n",
      "Successfully converted EP-15_ Behind_the_Tech_Kevin_Scott_Podcast_with_danah_boyd.docx to EP-15_ Behind_the_Tech_Kevin_Scott_Podcast_with_danah_boyd.txt\n",
      "Successfully converted EP-16_ Behind_the_Tech_Kevin_Scott_Podcast_with_Danny_Hillis.docx to EP-16_ Behind_the_Tech_Kevin_Scott_Podcast_with_Danny_Hillis.txt\n",
      "Successfully converted EP-17_ Behind_the_Tech_Kevin_Scott_Podcast_2019-Year_In_Review.docx to EP-17_ Behind_the_Tech_Kevin_Scott_Podcast_2019-Year_In_Review.txt\n",
      "Successfully converted EP-19_ Behind_the_Tech_Kevin_Scott_Podcast_PercyLiang.docx to EP-19_ Behind_the_Tech_Kevin_Scott_Podcast_PercyLiang.txt\n",
      "Successfully converted EP-20_ Behind_the_Tech_Kevin_Scott_Podcast_ReprogrammingTheDream.docx to EP-20_ Behind_the_Tech_Kevin_Scott_Podcast_ReprogrammingTheDream.txt\n",
      "Successfully converted EP-21_ Behind_the_Tech_Kevin_Eric-Horvitz_Chief-Scientific-Officer_Microsoft.docx to EP-21_ Behind_the_Tech_Kevin_Eric-Horvitz_Chief-Scientific-Officer_Microsoft.txt\n",
      "Successfully converted EP-22_ Behind_the_Tech_Kevin_Drew-Endy_Bioengineering-pioneer.docx to EP-22_ Behind_the_Tech_Kevin_Drew-Endy_Bioengineering-pioneer.txt\n",
      "Successfully converted EP-23_ Behind_the_Tech_Kevin_Tom-Daniel_Neuroscientist_and_bioengineer.docx to EP-23_ Behind_the_Tech_Kevin_Tom-Daniel_Neuroscientist_and_bioengineer.txt\n",
      "Successfully converted EP-24_ Behind_the_Tech_Kevin_Daphne-Koller_CEO_insitro.docx to EP-24_ Behind_the_Tech_Kevin_Daphne-Koller_CEO_insitro.txt\n",
      "Successfully converted EP-25_ Behind_the_Tech_CharlieStross_Sci-fi_author.docx to EP-25_ Behind_the_Tech_CharlieStross_Sci-fi_author.txt\n",
      "Successfully converted EP-26_ Behind_the_Tech_OrenEtzioni_CEO-AllenInstitute.docx to EP-26_ Behind_the_Tech_OrenEtzioni_CEO-AllenInstitute.txt\n",
      "Successfully converted EP-27_ Behind_the_Tech_2020-Year-In-Review.docx to EP-27_ Behind_the_Tech_2020-Year-In-Review.txt\n",
      "Successfully converted EP-28_Behind_the_Tech_Mae-Jemison.docx to EP-28_Behind_the_Tech_Mae-Jemison.txt\n",
      "Successfully converted EP-40_Behind_the_Tech_DanielaRus_Transcript (1).docx to EP-40_Behind_the_Tech_DanielaRus_Transcript (1).txt\n",
      "Successfully converted Ep-41-Behind_The_Tech-Neal_Stephenson-FINAL_2.docx to Ep-41-Behind_The_Tech-Neal_Stephenson-FINAL_2.txt\n",
      "Successfully converted Ep-44_Behind_the_Tech_Kevin Scott_Podcast_with_David_Baszucki.docx to Ep-44_Behind_the_Tech_Kevin Scott_Podcast_with_David_Baszucki.txt\n",
      "Successfully converted Ep-45_Behind_the_Tech_Kevin Scott_Podcast_with_Phil_Spencer.docx to Ep-45_Behind_the_Tech_Kevin Scott_Podcast_with_Phil_Spencer.txt\n",
      "Successfully converted Ep-46_Behind_the_Tech_Kevin_Scott_Podcast_with_Simone-Giertz.docx to Ep-46_Behind_the_Tech_Kevin_Scott_Podcast_with_Simone-Giertz.txt\n",
      "Successfully converted Ep-47_Behind_the_Tech_Kevin_Scott_Podcast_with_Sam_Schillace.docx to Ep-47_Behind_the_Tech_Kevin_Scott_Podcast_with_Sam_Schillace.txt\n",
      "Successfully converted EP-48_Behind_the_Tech_Kevin_Scott_Podcast_with_Randall_Munroe.docx to EP-48_Behind_the_Tech_Kevin_Scott_Podcast_with_Randall_Munroe.txt\n",
      "Successfully converted EP-49_Behind_the_Tech_Kevin_Scott_Podcast_2022YearinReview.docx to EP-49_Behind_the_Tech_Kevin_Scott_Podcast_2022YearinReview.txt\n",
      "Successfully converted EP-50_Behind_the_Tech_Kevin_Scott_Podcast_with_Tobi_Lutke.docx to EP-50_Behind_the_Tech_Kevin_Scott_Podcast_with_Tobi_Lutke.txt\n",
      "Successfully converted EP-51_Behind_the_Tech_Kevin Scott_Podcast_with_Rana_el_Kaliouby.docx to EP-51_Behind_the_Tech_Kevin Scott_Podcast_with_Rana_el_Kaliouby.txt\n",
      "Successfully converted Ep-52_Behind_the_Tech_Kevin_Scott_Podcast_with_Bill_Gates-qc.docx to Ep-52_Behind_the_Tech_Kevin_Scott_Podcast_with_Bill_Gates-qc.txt\n",
      "Successfully converted Ep-53_Behind_the_Tech_Kevin_Scott_Podcast_with_Adrian_Tchaikovsky.docx to Ep-53_Behind_the_Tech_Kevin_Scott_Podcast_with_Adrian_Tchaikovsky.txt\n",
      "Successfully converted Ep-54_Behind_the_Tech_Kevin_Scott_Podcast_with_Neil_deGrasse_Tyson_V2.docx to Ep-54_Behind_the_Tech_Kevin_Scott_Podcast_with_Neil_deGrasse_Tyson_V2.txt\n",
      "Successfully converted Ep-55_Behind_the_Tech_Kevin_Scott_Podcast_with_Mira_Murati.docx to Ep-55_Behind_the_Tech_Kevin_Scott_Podcast_with_Mira_Murati.txt\n",
      "Successfully converted Ep-56_Behind_the_Tech_Kevin_Scott_Podcast_with_will.i.am.docx to Ep-56_Behind_the_Tech_Kevin_Scott_Podcast_with_will.i.am.txt\n",
      "Successfully converted Ep-57_Behind_the_Tech_Kevin_Scott_Podcast_with_Kevin_Roose.docx to Ep-57_Behind_the_Tech_Kevin_Scott_Podcast_with_Kevin_Roose.txt\n",
      "Successfully converted Ep-58_Behind_the_Tech_Kevin_Scott_Podcast_with_William_A_Adams.docx to Ep-58_Behind_the_Tech_Kevin_Scott_Podcast_with_William_A_Adams.txt\n",
      "Successfully converted Ep-59_Behind_the_Tech_Kevin_Scott_Podcast_with_David_Kirtley.docx to Ep-59_Behind_the_Tech_Kevin_Scott_Podcast_with_David_Kirtley.txt\n",
      "Successfully converted Ep-60_Behind_the_Tech_Kevin_Scott_Podcast_2023_Year_in_Review.docx to Ep-60_Behind_the_Tech_Kevin_Scott_Podcast_2023_Year_in_Review.txt\n",
      "Successfully converted Ep-61_Behind_the_Tech_Kevin_Scott_Podcast_with_Lisa_Su.docx to Ep-61_Behind_the_Tech_Kevin_Scott_Podcast_with_Lisa_Su.txt\n",
      "Successfully converted Ep-62_Behind_the_Tech_Kevin_Scott_Podcast_with_Xyla_Foxlin.docx to Ep-62_Behind_the_Tech_Kevin_Scott_Podcast_with_Xyla_Foxlin.txt\n",
      "Successfully converted Ep-63_Behind_the_Tech_Kevin Scott_Podcast_with_Mike_Schroepfer.docx to Ep-63_Behind_the_Tech_Kevin Scott_Podcast_with_Mike_Schroepfer.txt\n",
      "Successfully converted Ep-64_Behind_the_Tech_Kevin Scott_Podcast_with_Mike_Volpi.docx to Ep-64_Behind_the_Tech_Kevin Scott_Podcast_with_Mike_Volpi.txt\n",
      "Successfully converted Ep-65_Behind_the_Tech_Kevin Scott_Podcast_with_Ethan_Mollick.docx to Ep-65_Behind_the_Tech_Kevin Scott_Podcast_with_Ethan_Mollick.txt\n",
      "Successfully converted Ep-66_Behind_the_Tech_Kevin Scott_Podcast_with_Sal_Khan_v1.docx to Ep-66_Behind_the_Tech_Kevin Scott_Podcast_with_Sal_Khan_v1.txt\n",
      "Successfully converted Ep-67_Behind_the_Tech_Kevin_Scott_Podcast_with_Ben_Laude.docx to Ep-67_Behind_the_Tech_Kevin_Scott_Podcast_with_Ben_Laude.txt\n",
      "Successfully converted EP-9_ Behind_the_Tech_Kevin Scott_Podcast_with_Reid_Hoffman.docx to EP-9_ Behind_the_Tech_Kevin Scott_Podcast_with_Reid_Hoffman.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import docx\n",
    "import PyPDF2\n",
    "\n",
    "# Create output folder if it doesn't exist\n",
    "Path('podcasttxtdata').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Process all files in input folder\n",
    "for filename in os.listdir('podcastdata'):\n",
    "    input_path = os.path.join('podcastdata', filename)\n",
    "    \n",
    "    # Skip if it's not a file\n",
    "    if not os.path.isfile(input_path):\n",
    "        continue\n",
    "        \n",
    "    # Get the file extension\n",
    "    file_extension = filename.lower().split('.')[-1]\n",
    "    \n",
    "    # Create output filename (replace extension with .txt)\n",
    "    output_filename = '.'.join(filename.split('.')[:-1]) + '.txt'\n",
    "    output_path = os.path.join('podcasttxtdata', output_filename)\n",
    "    \n",
    "    try:\n",
    "        # Process PDF files\n",
    "        if file_extension == 'pdf':\n",
    "            with open(input_path, 'rb') as file:\n",
    "                pdf_reader = PyPDF2.PdfReader(file)\n",
    "                text = ''\n",
    "                for page in pdf_reader.pages:\n",
    "                    text += page.extract_text()\n",
    "        \n",
    "        # Process DOCX files\n",
    "        elif file_extension in ['docx', 'doc']:\n",
    "            doc = docx.Document(input_path)\n",
    "            text = '\\n'.join([paragraph.text for paragraph in doc.paragraphs])\n",
    "        \n",
    "        # Save the extracted text\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(text)\n",
    "        print(f\"Successfully converted {filename} to {output_filename}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {filename}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting tokens in files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 62/62 [00:01<00:00, 60.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Token count for each file (sorted from smallest to largest):\n",
      "Behind-the-Tech_EP-38_StevenPinker_Transcript.txt: 0 tokens\n",
      "Behind-the-Tech_EP39_2021_Year-in-Review_Transcript.txt: 0 tokens\n",
      "EP-00_Behind_the_Tech_Kevin Scott_Podcast_Trailer.txt: 420 tokens\n",
      "EP-20_ Behind_the_Tech_Kevin_Scott_Podcast_ReprogrammingTheDream.txt: 8200 tokens\n",
      "Behind the Tech_Episode 30_Collier_Bloomberg_Transcript.txt: 8770 tokens\n",
      "Ep-52_Behind_the_Tech_Kevin_Scott_Podcast_with_Bill_Gates-qc.txt: 9288 tokens\n",
      "EP-28_Behind_the_Tech_Mae-Jemison.txt: 9618 tokens\n",
      "Behind the Tech_Episode 35_StevenBathiche_Transcript.txt: 9825 tokens\n",
      "EP-04_ Behind_the_Tech_Kevin Scott_Podcast_with_Judy Estrin.txt: 10106 tokens\n",
      "EP-40_Behind_the_Tech_DanielaRus_Transcript (1).txt: 10370 tokens\n",
      "EP-02_ Behind_the_Tech_Kevin Scott_Podcast_with_Alice Steinglass.txt: 10636 tokens\n",
      "EP-49_Behind_the_Tech_Kevin_Scott_Podcast_2022YearinReview.txt: 10805 tokens\n",
      "EP-03_ Behind_the_Tech_Kevin Scott_Podcast_with_Andrew Ng.txt: 10940 tokens\n",
      "EP-24_ Behind_the_Tech_Kevin_Daphne-Koller_CEO_insitro.txt: 11333 tokens\n",
      "Ep-44_Behind_the_Tech_Kevin Scott_Podcast_with_David_Baszucki.txt: 11528 tokens\n",
      "EP-06_ Behind_the_Tech_Kevin Scott_Podcast_with_Wences_Casares_TRANSCRIPT.txt: 11611 tokens\n",
      "Ep-41-Behind_The_Tech-Neal_Stephenson-FINAL_2.txt: 11664 tokens\n",
      "Behind the Tech_Episode 32_KimberlyBryant_Transcript.txt: 12071 tokens\n",
      "Ep-55_Behind_the_Tech_Kevin_Scott_Podcast_with_Mira_Murati.txt: 12104 tokens\n",
      "EP-21_ Behind_the_Tech_Kevin_Eric-Horvitz_Chief-Scientific-Officer_Microsoft.txt: 12122 tokens\n",
      "EP-01_ Behind_the_Tech_Kevin Scott_Podcast_with_Anders Hejlsberg.txt: 12272 tokens\n",
      "Ep-57_Behind_the_Tech_Kevin_Scott_Podcast_with_Kevin_Roose.txt: 12340 tokens\n",
      "Behind the Tech_Episode 34_ChaseJarvis_Transcript.txt: 12466 tokens\n",
      "EP-23_ Behind_the_Tech_Kevin_Tom-Daniel_Neuroscientist_and_bioengineer.txt: 12475 tokens\n",
      "Ep-61_Behind_the_Tech_Kevin_Scott_Podcast_with_Lisa_Su.txt: 12553 tokens\n",
      "EP-51_Behind_the_Tech_Kevin Scott_Podcast_with_Rana_el_Kaliouby.txt: 12644 tokens\n",
      "Behind the Tech_Episode 31_AshleyLlorens_Transcript.txt: 12655 tokens\n",
      "EP-08_ Behind_the_Tech_Kevin Scott_Podcast_with_Jason_Lanier_TRANSCRIPT.txt: 12824 tokens\n",
      "EP-50_Behind_the_Tech_Kevin_Scott_Podcast_with_Tobi_Lutke.txt: 12912 tokens\n",
      "EP-07_ Behind_the_Tech_Kevin Scott_Podcast_with_Dio_Gonzalez_TRANSCRIPT.txt: 12919 tokens\n",
      "EP-26_ Behind_the_Tech_OrenEtzioni_CEO-AllenInstitute.txt: 13343 tokens\n",
      "EP-19_ Behind_the_Tech_Kevin_Scott_Podcast_PercyLiang.txt: 13389 tokens\n",
      "EP-25_ Behind_the_Tech_CharlieStross_Sci-fi_author.txt: 13660 tokens\n",
      "EP-11_ Behind_the_Tech_Kevin Scott_Podcast_with_Fei-Fei-Li.txt: 13724 tokens\n",
      "EP-05_ Behind_the_Tech_Kevin Scott_Podcast_with_Danielle Feinberg.txt: 13725 tokens\n",
      "Ep-46_Behind_the_Tech_Kevin_Scott_Podcast_with_Simone-Giertz.txt: 13861 tokens\n",
      "EP-15_ Behind_the_Tech_Kevin_Scott_Podcast_with_danah_boyd.txt: 14090 tokens\n",
      "Behind the Tech_Episode 36_PeterLee_Transcript.txt: 14136 tokens\n",
      "Ep-60_Behind_the_Tech_Kevin_Scott_Podcast_2023_Year_in_Review.txt: 14237 tokens\n",
      "Ep-59_Behind_the_Tech_Kevin_Scott_Podcast_with_David_Kirtley.txt: 14473 tokens\n",
      "Ep-56_Behind_the_Tech_Kevin_Scott_Podcast_with_will.i.am.txt: 14537 tokens\n",
      "Ep-45_Behind_the_Tech_Kevin Scott_Podcast_with_Phil_Spencer.txt: 14592 tokens\n",
      "Ep-53_Behind_the_Tech_Kevin_Scott_Podcast_with_Adrian_Tchaikovsky.txt: 14865 tokens\n",
      "Ep-65_Behind_the_Tech_Kevin Scott_Podcast_with_Ethan_Mollick.txt: 14961 tokens\n",
      "EP-27_ Behind_the_Tech_2020-Year-In-Review.txt: 15282 tokens\n",
      "Ep-64_Behind_the_Tech_Kevin Scott_Podcast_with_Mike_Volpi.txt: 15293 tokens\n",
      "EP-14_ Behind_the_Tech_Kevin_Scott_Podcast_with_Neha_Narkhede.txt: 15312 tokens\n",
      "EP-12_Behind_the_Tech_Kevin Scott_Podcast_with_Bill-Coughran_TRANSCRIPT.txt: 15493 tokens\n",
      "EP-17_ Behind_the_Tech_Kevin_Scott_Podcast_2019-Year_In_Review.txt: 15692 tokens\n",
      "EP-9_ Behind_the_Tech_Kevin Scott_Podcast_with_Reid_Hoffman.txt: 15878 tokens\n",
      "EP-13_ Behind_the_Tech_Kevin_Scott_Podcast_with_Sam-Altman.txt: 15970 tokens\n",
      "EP-22_ Behind_the_Tech_Kevin_Drew-Endy_Bioengineering-pioneer.txt: 16207 tokens\n",
      "Ep-62_Behind_the_Tech_Kevin_Scott_Podcast_with_Xyla_Foxlin.txt: 16272 tokens\n",
      "Ep-58_Behind_the_Tech_Kevin_Scott_Podcast_with_William_A_Adams.txt: 16380 tokens\n",
      "Ep-47_Behind_the_Tech_Kevin_Scott_Podcast_with_Sam_Schillace.txt: 16679 tokens\n",
      "EP-48_Behind_the_Tech_Kevin_Scott_Podcast_with_Randall_Munroe.txt: 16774 tokens\n",
      "Ep-63_Behind_the_Tech_Kevin Scott_Podcast_with_Mike_Schroepfer.txt: 16897 tokens\n",
      "Ep-66_Behind_the_Tech_Kevin Scott_Podcast_with_Sal_Khan_v1.txt: 17367 tokens\n",
      "Ep-54_Behind_the_Tech_Kevin_Scott_Podcast_with_Neil_deGrasse_Tyson_V2.txt: 17933 tokens\n",
      "Behind the Tech_Episode 37_JustineEzarik_Transcript.txt: 18033 tokens\n",
      "EP-16_ Behind_the_Tech_Kevin_Scott_Podcast_with_Danny_Hillis.txt: 18982 tokens\n",
      "Ep-67_Behind_the_Tech_Kevin_Scott_Podcast_with_Ben_Laude.txt: 20957 tokens\n",
      "\n",
      "Total number of files: 62\n",
      "\n",
      "Copying 10 smallest files to ./ragtest/input/...\n",
      "Copied Behind-the-Tech_EP-38_StevenPinker_Transcript.txt (0 tokens)\n",
      "Copied Behind-the-Tech_EP39_2021_Year-in-Review_Transcript.txt (0 tokens)\n",
      "Copied EP-00_Behind_the_Tech_Kevin Scott_Podcast_Trailer.txt (420 tokens)\n",
      "Copied EP-20_ Behind_the_Tech_Kevin_Scott_Podcast_ReprogrammingTheDream.txt (8200 tokens)\n",
      "Copied Behind the Tech_Episode 30_Collier_Bloomberg_Transcript.txt (8770 tokens)\n",
      "Copied Ep-52_Behind_the_Tech_Kevin_Scott_Podcast_with_Bill_Gates-qc.txt (9288 tokens)\n",
      "Copied EP-28_Behind_the_Tech_Mae-Jemison.txt (9618 tokens)\n",
      "Copied Behind the Tech_Episode 35_StevenBathiche_Transcript.txt (9825 tokens)\n",
      "Copied EP-04_ Behind_the_Tech_Kevin Scott_Podcast_with_Judy Estrin.txt (10106 tokens)\n",
      "Copied EP-40_Behind_the_Tech_DanielaRus_Transcript (1).txt (10370 tokens)\n",
      "\n",
      "Done! Summary of copied files:\n",
      "Total files copied: 10\n",
      "Total tokens in copied files: 66597\n",
      "Average tokens per file: 6659\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import tiktoken\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Initialize the GPT-4 tokenizer\n",
    "tokenizer = tiktoken.get_encoding(\"o200k_base\")  # This is the encoding used by gpt-4o, gpt-4o-mini\n",
    "# tokenizer = tiktoken.encoding_for_model(\"gpt-4o-mini\")\n",
    "\n",
    "def count_tokens(text):\n",
    "    \"\"\"Count tokens in text using GPT-4 tokenizer\"\"\"\n",
    "    return len(tokenizer.encode(text))\n",
    "\n",
    "# Create dictionary to store file paths and their token counts\n",
    "file_token_counts = {}\n",
    "\n",
    "# Process all txt files\n",
    "print(\"Counting tokens in files...\")\n",
    "for filename in tqdm(os.listdir('podcasttxtdata')):\n",
    "    if filename.endswith('.txt'):\n",
    "        filepath = os.path.join('podcasttxtdata', filename)\n",
    "        try:\n",
    "            # Read the text file\n",
    "            with open(filepath, 'r', encoding='utf-8') as f:\n",
    "                text = f.read()\n",
    "            \n",
    "            # Count tokens\n",
    "            token_count = count_tokens(text)\n",
    "            \n",
    "            # Store filepath and token count\n",
    "            file_token_counts[filepath] = token_count\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {str(e)}\")\n",
    "\n",
    "# Sort files by token count\n",
    "sorted_files = sorted(file_token_counts.items(), key=lambda x: x[1])\n",
    "\n",
    "# Print summary of all files and their token counts\n",
    "print(\"\\nToken count for each file (sorted from smallest to largest):\")\n",
    "for filepath, count in sorted_files:\n",
    "    filename = os.path.basename(filepath)\n",
    "    print(f\"{filename}: {count} tokens\")\n",
    "\n",
    "# Create ragtest/input directory if it doesn't exist\n",
    "ragtest_dir = Path('./ragtest/input')\n",
    "ragtest_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Ask user how many files they want to copy\n",
    "total_files = len(sorted_files)\n",
    "print(f\"\\nTotal number of files: {total_files}\")\n",
    "num_files = int(input(\"How many of the smallest files do you want to copy to ragtest/input? \"))\n",
    "\n",
    "# Copy the smallest files\n",
    "print(f\"\\nCopying {num_files} smallest files to ./ragtest/input/...\")\n",
    "for filepath, count in sorted_files[:num_files]:\n",
    "    filename = os.path.basename(filepath)\n",
    "    destination = os.path.join(ragtest_dir, filename)\n",
    "    shutil.copy2(filepath, destination)\n",
    "    print(f\"Copied {filename} ({count} tokens)\")\n",
    "\n",
    "print(\"\\nDone! Summary of copied files:\")\n",
    "total_tokens = sum(count for _, count in sorted_files[:num_files])\n",
    "print(f\"Total files copied: {num_files}\")\n",
    "print(f\"Total tokens in copied files: {total_tokens}\")\n",
    "print(f\"Average tokens per file: {total_tokens // num_files}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5742518431761192"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " (806465 /66597) * 0.13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "12.109629562893224 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done! Summary of  files:\n",
      "Total files : 62\n",
      "Total tokens in files: 806465\n",
      "Average tokens per file: 13007\n"
     ]
    }
   ],
   "source": [
    "num_files = len(sorted_files)\n",
    "print(\"\\nDone! Summary of  files:\")\n",
    "total_tokens = sum(count for _, count in sorted_files[:num_files])\n",
    "print(f\"Total files : {num_files}\")\n",
    "print(f\"Total tokens in files: {total_tokens}\")\n",
    "print(f\"Average tokens per file: {total_tokens // num_files}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from neo4j import GraphDatabase\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Specify the exact path to your .env file\n",
    "ENV_PATH = r\"./ragtest/.env\"\n",
    "load_dotenv(ENV_PATH)\n",
    "NEO4J_URI=\"neo4j+s://c55a8a58.databases.neo4j.io\"\n",
    "NEO4J_USERNAME=\"neo4j\"\n",
    "NEO4J_PASSWORD=os.getenv(\"NEO4J_PASSWORD\") \n",
    "NEO4J_DATABASE=\"neo4j\"\n",
    "driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USERNAME, NEO4J_PASSWORD))\n",
    "\n",
    "def db_query(cypher: str, params: Dict[str, Any] = {}) -> pd.DataFrame:\n",
    "    \"\"\"Executes a Cypher statement and returns a DataFrame\"\"\"\n",
    "    return driver.execute_query(\n",
    "        cypher, parameters_=params, result_transformer_=Result.to_df\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_query(\n",
    "  \"MATCH (n:__Chunk__) RETURN n.n_tokens as token_count, count(*) AS count\"\n",
    ")\n",
    "# token_count count\n",
    "# 300         230\n",
    "# 155         1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_query(\n",
    "  \"MATCH (n:__Chunk__) RETURN n.n_tokens as token_count, count(*) AS count\"\n",
    ")\n",
    "# token_count count\n",
    "# 300         230\n",
    "# 155         1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_query(\n",
    "  \"MATCH (n:__Chunk__) RETURN n.n_tokens as token_count, count(*) AS count\"\n",
    ")\n",
    "# token_count count\n",
    "# 300         230\n",
    "# 155         1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_query(\"\"\"\n",
    "  MATCH (n:__Community__) \n",
    "  RETURN n.title AS title, n.summary AS summary, n.full_content AS full_content LIMIT 1\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_df = db_query(\n",
    "    \"\"\"\n",
    "MATCH (d:__Chunk__)\n",
    "RETURN count {(d)-[:HAS_ENTITY]->()} AS entity_count\n",
    "\"\"\"\n",
    ")\n",
    "# Plot distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(entity_df['entity_count'], kde=True, bins=15, color='skyblue')\n",
    "plt.axvline(entity_df['entity_count'].mean(), color='red', linestyle='dashed', linewidth=1)\n",
    "plt.axvline(entity_df['entity_count'].median(), color='green', linestyle='dashed', linewidth=1)\n",
    "plt.xlabel('Entity Count', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.title('Distribution of Entity Count', fontsize=15)\n",
    "plt.legend({'Mean': entity_df['entity_count'].mean(), 'Median': entity_df['entity_count'].median()})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_dist_df = db_query(\n",
    "    \"\"\"\n",
    "MATCH (e:__Entity__)\n",
    "RETURN count {(e)-[:RELATED]-()} AS node_degree\n",
    "\"\"\"\n",
    ")\n",
    "# Calculate mean and median\n",
    "mean_degree = np.mean(degree_dist_df['node_degree'])\n",
    "percentiles = np.percentile(degree_dist_df['node_degree'], [25, 50, 75, 90])\n",
    "# Create a histogram with a logarithmic scale\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(degree_dist_df['node_degree'], bins=50, kde=False, color='blue')\n",
    "# Use a logarithmic scale for the x-axis\n",
    "plt.yscale('log')\n",
    "# Adding labels and title\n",
    "plt.xlabel('Node Degree')\n",
    "plt.ylabel('Count (log scale)')\n",
    "plt.title('Node Degree Distribution')\n",
    "# Add mean, median, and percentile lines\n",
    "plt.axvline(mean_degree, color='red', linestyle='dashed', linewidth=1, label=f'Mean: {mean_degree:.2f}')\n",
    "plt.axvline(percentiles[0], color='purple', linestyle='dashed', linewidth=1, label=f'25th Percentile: {percentiles[0]:.2f}')\n",
    "plt.axvline(percentiles[1], color='orange', linestyle='dashed', linewidth=1, label=f'50th Percentile: {percentiles[1]:.2f}')\n",
    "plt.axvline(percentiles[2], color='yellow', linestyle='dashed', linewidth=1, label=f'75th Percentile: {percentiles[2]:.2f}')\n",
    "plt.axvline(percentiles[3], color='brown', linestyle='dashed', linewidth=1, label=f'90th Percentile: {percentiles[3]:.2f}')\n",
    "# Add legend\n",
    "plt.legend()\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_query(\"\"\"\n",
    "  MATCH (n:__Entity__) \n",
    "  RETURN n.name AS name, count{(n)-[:RELATED]-()} AS degree\n",
    "  ORDER BY degree DESC LIMIT 5\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "community_data = db_query(\"\"\"\n",
    "  MATCH (n:__Community__)\n",
    "  RETURN n.level AS level, count{(n)-[:IN_COMMUNITY]-()} AS members\n",
    "\"\"\")\n",
    "\n",
    "stats = community_data.groupby('level').agg(\n",
    "    min_members=('members', 'min'),\n",
    "    max_members=('members', 'max'),\n",
    "    median_members=('members', 'median'),\n",
    "    avg_members=('members', 'mean'),\n",
    "    num_communities=('members', 'count'),\n",
    "    total_members=('members', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "# Create box plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='level', y='members', data=community_data, palette='viridis')\n",
    "plt.xlabel('Level')\n",
    "plt.ylabel('Members')\n",
    "\n",
    "# Add statistical annotations\n",
    "for i in range(stats.shape[0]):\n",
    "    level = stats['level'][i]\n",
    "    max_val = stats['max_members'][i]\n",
    "    text = (f\"num: {stats['num_communities'][i]}\\n\"\n",
    "            f\"all_members: {stats['total_members'][i]}\\n\"\n",
    "            f\"min: {stats['min_members'][i]}\\n\"\n",
    "            f\"max: {stats['max_members'][i]}\\n\"\n",
    "            f\"med: {stats['median_members'][i]}\\n\"\n",
    "            f\"avg: {stats['avg_members'][i]:.2f}\")\n",
    "    plt.text(level, 85, text, horizontalalignment='center', fontsize=9)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"entity\"\n",
    "\n",
    "db_query(\n",
    "    \"\"\"\n",
    "CREATE VECTOR INDEX \"\"\"\n",
    "    + index_name\n",
    "    + \"\"\" IF NOT EXISTS FOR (e:__Entity__) ON e.description_embedding\n",
    "OPTIONS {indexConfig: {\n",
    " `vector.dimensions`: 1536,\n",
    " `vector.similarity_function`: 'cosine'\n",
    "}}\n",
    "\"\"\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
