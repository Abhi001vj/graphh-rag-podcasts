{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment variables loaded successfully from ./ragtest/.env!\n"
     ]
    }
   ],
   "source": [
    "# First cell - Load environment variables from specific path\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Specify the exact path to your .env file\n",
    "ENV_PATH = r\"./ragtest/.env\"\n",
    "\n",
    "# Load .env file from specific path\n",
    "if not os.path.exists(ENV_PATH):\n",
    "    raise FileNotFoundError(f\".env file not found at {ENV_PATH}\")\n",
    "\n",
    "load_dotenv(ENV_PATH)\n",
    "\n",
    "# Verify API key loaded\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    raise ValueError(\"OPENAI_API_KEY not found in .env file\")\n",
    "\n",
    "print(f\"Environment variables loaded successfully from {ENV_PATH}!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new index...\n",
      "Loaded 10 documents\n",
      "Split into 307 chunks\n",
      "Creating vector store...\n",
      "Index saved to ./ragtest/faiss_index\n",
      "\n",
      "Index Statistics:\n",
      "Total number of chunks in index: 307\n",
      "\n",
      "Sample of first few document chunks (IDs):\n",
      "- 41af3c29-02ac-4d97-89b2-6c53b6c72fc4\n",
      "- e0abd438-468f-4258-8081-785a14210599\n",
      "- 50515279-8b79-4568-b83a-0bf4a5f8b640\n",
      "- ee357d3e-45ac-46c9-ac84-4349e0d4bb1f\n",
      "- e66d7bab-4a37-4bbe-801b-2b47b5729be0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Second cell - Import required libraries\n",
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define constants\n",
    "INPUT_DIR = \"./ragtest/input\"\n",
    "INDEX_DIR = \"./ragtest/faiss_index\"\n",
    "CACHE_DIR = \"./cache\"\n",
    "\n",
    "# Create necessary directories\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "os.makedirs(os.path.dirname(INDEX_DIR), exist_ok=True)\n",
    "\n",
    "# Third cell - Create or load index\n",
    "def create_or_load_index():\n",
    "    if os.path.exists(INDEX_DIR):\n",
    "        print(\"Loading existing index...\")\n",
    "        embeddings = OpenAIEmbeddings(\n",
    "            model=\"text-embedding-3-small\"\n",
    "        )\n",
    "        vector_store = FAISS.load_local(INDEX_DIR, embeddings)\n",
    "        return vector_store\n",
    "    \n",
    "    print(\"Creating new index...\")\n",
    "    loader = DirectoryLoader(\n",
    "        INPUT_DIR,\n",
    "        glob=\"**/*.txt\",\n",
    "        loader_cls=TextLoader,\n",
    "        loader_kwargs={'autodetect_encoding': True}\n",
    "    )\n",
    "    documents = loader.load()\n",
    "    print(f\"Loaded {len(documents)} documents\")\n",
    "    \n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1200,\n",
    "        chunk_overlap=100,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "    splits = text_splitter.split_documents(documents)\n",
    "    print(f\"Split into {len(splits)} chunks\")\n",
    "    \n",
    "    embeddings = OpenAIEmbeddings(\n",
    "        model=\"text-embedding-3-small\",\n",
    "    )\n",
    "    \n",
    "    print(\"Creating vector store...\")\n",
    "    vector_store = FAISS.from_documents(splits, embeddings)\n",
    "    vector_store.save_local(INDEX_DIR)\n",
    "    print(f\"Index saved to {INDEX_DIR}\")\n",
    "    \n",
    "    return vector_store\n",
    "\n",
    "# Create or load the index\n",
    "vector_store = create_or_load_index()\n",
    "\n",
    "# Fourth cell - Setup QA chain\n",
    "prompt_template = \"\"\"Use the following pieces of context to answer the question at the end. \n",
    "If you don't know the answer, just say that you don't know. Don't try to make up an answer.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer: Let me answer based on the specific context provided.\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "# Initialize LLM and QA chain\n",
    "llm = ChatOpenAI(\n",
    "    model_name=\"gpt-4o-mini\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=vector_store.as_retriever(\n",
    "        search_kwargs={\n",
    "            \"k\": 3,\n",
    "            \"fetch_k\": 5\n",
    "        }\n",
    "    ),\n",
    "    chain_type=\"stuff\",\n",
    "    chain_type_kwargs={\n",
    "        \"prompt\": PROMPT,\n",
    "        \"verbose\": True\n",
    "    }\n",
    ")\n",
    "\n",
    "# Fifth cell - Helper function to get document stats\n",
    "def get_document_stats():\n",
    "    doc_count = len(vector_store.docstore._dict)\n",
    "    print(f\"\\nIndex Statistics:\")\n",
    "    print(f\"Total number of chunks in index: {doc_count}\")\n",
    "    \n",
    "    sample_ids = list(vector_store.docstore._dict.keys())[:5]\n",
    "    print(f\"\\nSample of first few document chunks (IDs):\")\n",
    "    for id in sample_ids:\n",
    "        print(f\"- {id}\")\n",
    "\n",
    "# Show initial stats\n",
    "get_document_stats()\n",
    "\n",
    "# Sixth cell - Function to ask a single question\n",
    "def ask_question(question: str):\n",
    "    try:\n",
    "        result = qa_chain.invoke(question)\n",
    "        return result['result']\n",
    "    except Exception as e:\n",
    "        return f\"Error processing question: {str(e)}\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mUse the following pieces of context to answer the question at the end. \n",
      "If you don't know the answer, just say that you don't know. Don't try to make up an answer.\n",
      "\n",
      "Context: I think there are also a bunch of folks like me, where it's going to be all kinds of nostalgic.  You know just, \"Oh my God, I remember that.\"  Like the walk down memory lane. \n",
      "What I want people to get out of the show is inspiration.  I want them to be a little more informed about the history of computing, and what's happening now behind the scenes they might not otherwise see.  And I think all of that's important because technology is such a huge part of all of our daily lives, that even those of us who aren't engineers should understand a little bit more about what's going on behind the tech that is suffusing our lives. \n",
      "I'm Kevin Scott, Chief Technology Officer at Microsoft.  In my new podcast, I'll be talking with really cool people who have done and are still building really amazing technology.  So sign up and tune in so we geek out this summer.\n",
      "\n",
      "JUDY ESTRIN: (Voiceover) Right now, so much of what is going on in the industry is driven by the values of scale and speed. It is about maximizing growth. It is about maximizing connectivity. What about thinking about what we're losing in those values. There's nothing wrong with growth, but not at the expense of humanity. \n",
      "KEVIN SCOTT: Hi, everyone. Welcome to Behind the Tech. I'm your host, Kevin Scott, Chief Technology Officer for Microsoft. In this podcast, we're going to get behind the tech. We'll talk with some of the people who have made our modern tech world possible and understand what motivated them to create what they did. \n",
      "So, join me to maybe learn a little bit about the history of computing, and get a few behind-the-scenes insights into what's happening today. Stick around.\n",
      "\n",
      "[MUSIC]\n",
      "\n",
      "KEVIN SCOTT: In these podcasts episodes, we're going to get behind the tech.  \n",
      "We're going to talk with some of the people that have made our modern technological world possible, and understand what motivated them to do what they did.  They had a great idea.  They had passion about a particular thing and they did it, and it benefited all of us in this really awesome way, and shouldn't we care about that? \n",
      "I think history is valuable in a ton of different ways, like helping you not get too damned excited over the thing that you've just invented that you really didn't invent because it's been invented ten times already, and if you knew that, maybe you could have saved yourself just a little bit of work.  \n",
      "I'd love for developers of all ages to listen, so people who are early in their career to learn a little bit about the history of the industry, and maybe to get some insight and perspective on what it means to be a really great engineer.  Wow, it's great that I can see this person who, in a whole bunch of ways, is like me and maybe that inspires me to go off and do something awesome.\n",
      "\n",
      "Question: What topics are covered in these podcasts?\n",
      "\n",
      "Answer: Let me answer based on the specific context provided.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "The podcasts cover topics related to the history of computing, insights into modern technology, the motivations behind technological innovations, and the experiences of influential figures in the tech industry. They aim to inspire listeners, particularly developers, by providing a deeper understanding of the tech world and encouraging reflection on the values of growth and humanity in technology.\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "answer = ask_question(\"What topics are covered in these podcasts?\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mUse the following pieces of context to answer the question at the end. \n",
      "If you don't know the answer, just say that you don't know. Don't try to make up an answer.\n",
      "\n",
      "Context: Behind the Tech with Kevin Scott Podcast\n",
      "Episode 52 – Bill Gates\n",
      "\n",
      "BILL GATES: It was stunning, it was mind blowing. After the biology question I had them type in, “What do you say to a father with a sick child?” And it gave this very careful, excellent answer that was perhaps better than any of us in the room might have given. And so, it was like, wow, what is it – what is the scope of this thing? Because this is way better.\n",
      "[MUSIC]\n",
      "\n",
      "KEVIN SCOTT: Hi, everyone. Welcome to Behind the Tech. I’m your host, Kevin Scott, Chief Technology Officer for Microsoft. \n",
      "\n",
      "In this podcast, we’re going to get behind the tech. We’ll talk with some of the people who have made our modern tech world possible and understand what motivated them to create what they did. So, join me to maybe learn a little bit about the history of computing and get a few behind-the-scenes insights into what’s happening today. Stick around. \n",
      "\n",
      "[MUSIC]\n",
      "\n",
      "Uh, and even like what it felt like for him as one of the leaders helping bring the internet revolution to the world. Uh, yeah. So like those parts of the conversation today that we had where he was recounting some of his experiences, like the, uh, the first meeting that he had with Charles Simon at Xerox Park, uh, where he saw one of the world's first graphical word processors and like how, seeing that and talking with Charles influenced, uh, an enormous amount of the history of not just Microsoft, but the world in the, uh, subsequent years. Um, and, and just hearing Bill talk about his, uh, passion for the things that the Gates Foundation is doing and what these AI technologies mean for those things, like how, uh, maybe we can use these technologies to accelerate some of the benefits.\n",
      "To the people in the world who are most in need of technologies like this to help them live, uh, better and more successful lives. Um, so again, they, this is tremendous treat for, uh, being able to talk with Bill on the podcast today.\n",
      "\n",
      "BILL GATES: Well, if there hadn’t been this darn AI advance that’s distracting me. I’m kidding you, but it’s really true. I have allocated, and with super excitement, a lot more time to sitting with Microsoft product groups and saying, okay, what does this mean for security, what does it mean for Office, what does it mean for our database type thing? Because I love that type of brainstorming because new vistas are opened up. So, no, it’s all your fault, no Infinite Jest this year. \n",
      "\n",
      "KEVIN SCOTT: Excellent. Well, I appreciate you making that trade because it’s been really fantastic over the past six months, having you help us think through all of this stuff. So thank you for that and thank you for doing the podcast today. Really, really appreciate it. \n",
      "\n",
      "BILL GATES: No, it was fun. Kevin. Thanks so much. \n",
      "\n",
      "KEVIN SCOTT: Awesome. \n",
      "\n",
      "[MUSIC]\n",
      "\n",
      "Question: What are the episodes that talk about Billgates?\n",
      "\n",
      "Answer: Let me answer based on the specific context provided.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "The context provided mentions Episode 52 of the \"Behind the Tech with Kevin Scott\" podcast, which features Bill Gates as a guest. This episode discusses various topics, including Gates' experiences with technology and his insights on AI.\n"
     ]
    }
   ],
   "source": [
    "answer = ask_question(\"What are the episodes that talk about Billgates?\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
